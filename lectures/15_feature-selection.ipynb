{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 15: feature selection\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=YIGk_QCgm-A&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=14)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "discuss non-uniqueness of OLS solution / multicollinearity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n",
    "1. Crazy Dream by Tom Misch, Loyle Carner\n",
    "2. Big Iron by Marty Robbins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a3 in progress, due Wednesday\n",
    "- a3 bug/fix: https://edstem.org/us/courses/3226/discussion/248155\n",
    "- Countdown to reading week: 1 more class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- change of basis notation\n",
    "- finding the “true” model\n",
    "- complexity penalties and information criteria\n",
    "- feature selection intro\n",
    "- association approach\n",
    "- regression weight approach\n",
    "- search and score approach\n",
    "- L0 norm penalty\n",
    "- forward selection\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra discussion\n",
    "\n",
    "Finding the \"true\" model - what about non-uniqueness of the optimization problem? (This was discussed in lecture 12 but the whiteboard was unreadable in the video.)\n",
    "\n",
    "$\\hat{y}_i = w_0 + w_1^Tx_1 + w_2^Tx_2 + \\ldots + w_d^Tx_d$\n",
    "\n",
    "What if $x_1=x_2$ are the same for all training examples?\n",
    "\n",
    "Then in fact we have:\n",
    "\n",
    "$\\hat{y}_i = w_0 + w_1^Tx_1 + w_2^Tx_1 + \\ldots + w_d^Tx_d$\n",
    "\n",
    "or, \n",
    "\n",
    "$\\hat{y}_i = w_0 + (w_1 + w_2)^Tx_1 + \\ldots + w_d^Tx_d$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a problem because we can change $w_1$ and $w_2$ without changing $w_1+w_2$. \n",
    "- Thus, for any solution to the optimization problem (that minimizes the squared error objective), we can construct another solution be replacing $w_1$ with $w_1+a$ and $w_2$ with $w_2-a$.\n",
    "- So, we have infinitely many solutions to the optimization problem.\n",
    "- We can try to draw this in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(X, y):\n",
    "    plt.figure()\n",
    "    w1 = np.arange(-15, 15, 0.25)\n",
    "    w2 = np.arange(-15, 15, 0.25)\n",
    "    W1, W2 = np.meshgrid(w1, w2)\n",
    "    W = np.vstack((W1.flatten(), W2.flatten()))\n",
    "\n",
    "    loss = np.sum((X@W-y)**2, axis=0)\n",
    "    LOSS = np.reshape(loss, (len(w1), len(w2)))\n",
    "        \n",
    "    plt.imshow(LOSS, extent=(np.min(w1), np.max(w1), np.min(w2), np.max(w2)), aspect=\"auto\");\n",
    "    plt.xlabel('w1');\n",
    "    plt.ylabel('w2');\n",
    "    plt.colorbar();\n",
    "    plt.title('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e29f642c1ae475d9b5308032c609ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.rand(50,2)\n",
    "y = 2*X[:,0][:,None] + X[:,1][:,None] - 1\n",
    "plot_loss(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7552a5ca07c4625824030ea1073dc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xcopy = np.hstack((X[:,:1],X[:,:1]))\n",
    "plot_loss(Xcopy, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_3D(X, y):\n",
    "    w1 = np.arange(-10, 10, 0.25)\n",
    "    w2 = np.arange(-10, 10, 0.25)\n",
    "    W1, W2 = np.meshgrid(w1, w2)\n",
    "    W = np.vstack((W1.flatten(), W2.flatten()))\n",
    "\n",
    "    loss = np.sum((X@W-y)**2, axis=0)\n",
    "    LOSS = np.reshape(loss, (len(w1), len(w2)))\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    surf = ax.plot_surface(W1, W2, LOSS, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "    plt.xlabel('w1');\n",
    "    plt.ylabel('w2');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50956d904173421c9bd22767219c59d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_3D(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaad96203ac4c92806c0131b26d9d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_3D(Xcopy, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Simple association-based feature selection approaches do not take into account the interaction between features.\n",
    "2. You can carry out feature selection using linear models by pruning the features which have very small weights. \n",
    "3. Forward selection is guaranteed to find the best feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for least squares with the L0-norm penalty is $f(w) = \\frac{1}{2}\\lVert{Xw -y}\\rVert^2 + \\lambda \\lVert w\\rVert_0$\n",
    "\n",
    "1. We can minimize this loss function with the normal equations.\n",
    "2. We can minimize this loss function with gradient descent.\n",
    "3. Decreasing $\\lambda$ encourages the model to select more features.\n",
    "4. Decreasing $\\lVert w\\rVert_0$ encourages the model to select more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow up Q:\n",
    "\n",
    "Imagine we duplicated every example in the training set, thus doubling the number of rows in $X$ and $y$. We leave everything else the same. This may change the number of features selected when minimizing the above loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student questions\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/249407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
