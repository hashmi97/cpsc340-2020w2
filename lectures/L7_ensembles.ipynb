{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 7: Ensemble Methods\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=3SD6fgNGZSo&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's pre-class songs\n",
    "\n",
    "1. Heybb! by binki\n",
    "2. Heaven by The Walkmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a2 posted\n",
    "- a1 solution updated to include code\n",
    "- Survey results: https://edstem.org/us/courses/3226/discussion/218421\n",
    "  - You seem to be OK with breakout rooms ✔️\n",
    "  - You seem to be OK with recording the chat ✔️\n",
    "  - Your preference for how we spend class time seems to be:\n",
    "  1. Old exam questions\n",
    "  2. Video summary\n",
    "  3. True/False polls\n",
    "  4. Answering student questions\n",
    "- Countdown to reading week: 9 more classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- decision theory\n",
    "- classification metrics (bonus)\n",
    "- prominent hyperparameters\n",
    "- naive Bayes recap\n",
    "- ensembles - intro\n",
    "- averaging models\n",
    "- random forests\n",
    "  - average of a bunch of decision trees, but...\n",
    "  - each tree looks at a slightly different version of the dataset (bootstrap)\n",
    "  - each split (stump) only gets to choose from a random subset of the features\n",
    "- part 1 recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017W2 midterm\n",
    "\n",
    "Consider the classification boundary below, with the training data shown.\n",
    "\n",
    "![](img/L7/rf.png)\n",
    "\n",
    "Do you think this boundary was produced by a decision tree or a random forest? Briefly explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer**: random forest. Some of the splits shown here wouldn't make sense for a decision tree. Random forests are harder to understand/interpret so it's harder to say yes/no to random forest given the picture, but we can say \"no\" to decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "plot the decision boundaries from each of the random trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017S1 midterm\n",
    "\n",
    "Why do we create random forests out of random trees (trees where each stump only looks at a subset of the features, and the dataset is a bootstrap sample) rather than creating them out of regular decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** If we used regular decision trees, we'd get the same tree each time, and averaging would not accomplish anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "The following hyperparameter changes would result in **lower training error**:\n",
    "\n",
    "1. Increasing the number of features considered for each split in a random forest will tend to decrease training error.\n",
    "2. Increasing the number of trees in a random forest will tend to decrease training error.\n",
    "3. Let classifiers A, B, and C have training errors of 10%, 30%, and 30%, respectively. Then, the best possible training error from averaging A, B and C is 10%.\n",
    "4. Let classifiers A, B, and C all have 30% error. Then, the worst possible training error from averaging A, B and C is 30%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers**:\n",
    "\n",
    "1. True. Giving the trees more options means they should be able to better fit the training data.\n",
    "2. True. This is again making the model more complex which pushes the fundamental tradeoff towards lower training error.\n",
    "3. False. Surprisingly, adding in worse classifiers to a 10%-error-classifier can actually improve it! The power of averaging. \n",
    "4. False. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
